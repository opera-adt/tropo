{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "from opera_tropo import download\n",
    "\n",
    "\n",
    "class WeatherDataAnalyzer:\n",
    "    \"\"\"A class for analyzing weather data from NetCDF files and generating statistical reports.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: str = \"./weather_stats\",\n",
    "        n_workers: int = 4,\n",
    "        s3_profile: str = \"saml-pub\",\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize the WeatherDataAnalyzer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save output files\n",
    "        n_workers : int\n",
    "            Number of Dask workers\n",
    "        s3_profile : str\n",
    "            S3 profile name for authentication\n",
    "        verbose : bool\n",
    "            Whether to print progress messages\n",
    "\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.n_workers = n_workers\n",
    "        self.s3_profile = s3_profile\n",
    "        self.verbose = verbose\n",
    "        self.logger = self._setup_logging()\n",
    "\n",
    "        # Variable configurations\n",
    "        self.variable_configs = {\n",
    "            \"z\": {\n",
    "                \"levels\": [0],\n",
    "                \"desc\": \"Geopotential (surface)\",\n",
    "                \"check_negative\": False,\n",
    "            },\n",
    "            \"t\": {\n",
    "                \"levels\": \"all\",\n",
    "                \"desc\": \"Temperature (all levels)\",\n",
    "                \"check_negative\": False,\n",
    "            },\n",
    "            \"q\": {\n",
    "                \"levels\": \"all\",\n",
    "                \"desc\": \"Specific humidity (all levels)\",\n",
    "                \"check_negative\": True,\n",
    "            },\n",
    "            \"lnsp\": {\n",
    "                \"levels\": [0],\n",
    "                \"desc\": \"Log surface pressure\",\n",
    "                \"check_negative\": False,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _setup_logging(self) -> logging.Logger:\n",
    "        \"\"\"Set up logging configuration.\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            logger.setLevel(logging.INFO if self.verbose else logging.WARNING)\n",
    "        return logger\n",
    "\n",
    "    def _extract_time_info(self, dataset: xr.Dataset) -> Tuple[str, str]:\n",
    "        \"\"\"Extract model date and time information from dataset.\"\"\"\n",
    "        try:\n",
    "            if \"time\" in dataset.coords:\n",
    "                model_time = pd.to_datetime(dataset.time.values[0])\n",
    "                return (\n",
    "                    model_time.strftime(\"%Y-%m-%d\"),\n",
    "                    model_time.strftime(\"%H:%M:%S UTC\"),\n",
    "                )\n",
    "            else:\n",
    "                self.logger.warning(\"No time coordinate found in dataset\")\n",
    "                return \"Unknown\", \"Unknown\"\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error extracting time info: {e}\")\n",
    "            return \"Unknown\", \"Unknown\"\n",
    "\n",
    "    def _get_variable_data(\n",
    "        self, dataset: xr.Dataset, var: str, config: Dict[str, Any]\n",
    "    ) -> Optional[xr.DataArray]:\n",
    "        \"\"\"Extract variable data based on configuration.\"\"\"\n",
    "        if var not in dataset.data_vars:\n",
    "            self.logger.warning(f\"Variable '{var}' not found in dataset\")\n",
    "            return None\n",
    "\n",
    "        data = dataset[var]\n",
    "\n",
    "        # Apply level selection if specified\n",
    "        if config[\"levels\"] != \"all\" and \"level\" in data.dims:\n",
    "            try:\n",
    "                data = data.isel(level=config[\"levels\"])\n",
    "            except (IndexError, KeyError) as e:\n",
    "                self.logger.warning(f\"Error selecting levels for {var}: {e}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _compute_statistics(\n",
    "        self, data: xr.DataArray, var: str, config: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Compute comprehensive statistics for a variable.\"\"\"\n",
    "        stats = {\n",
    "            f\"{var}_min\": data.min(),\n",
    "            f\"{var}_max\": data.max(),\n",
    "            f\"{var}_mean\": data.mean(),\n",
    "            f\"{var}_std\": data.std(),\n",
    "            f\"{var}_total_size\": data.size,\n",
    "            f\"{var}_nan_count\": data.isnull().sum(),\n",
    "            f\"{var}_zero_count\": (data == 0.0).sum(),\n",
    "            f\"{var}_inf_count\": np.isinf(data).sum(),\n",
    "            f\"{var}_finite_count\": np.isfinite(data).sum(),\n",
    "        }\n",
    "\n",
    "        # Add negative count only for variables where it's relevant\n",
    "        if config[\"check_negative\"]:\n",
    "            stats[f\"{var}_negative_count\"] = (data < 0).sum()\n",
    "        else:\n",
    "            stats[f\"{var}_negative_count\"] = xr.DataArray(0)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _create_quality_flags(\n",
    "        self, var: str, results: Dict[str, Any], config: Dict[str, Any]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Create quality flags based on computed statistics.\"\"\"\n",
    "        flags = []\n",
    "\n",
    "        nan_count = results[f\"{var}_nan_count\"].item()\n",
    "        inf_count = results[f\"{var}_inf_count\"].item()\n",
    "        zero_count = results[f\"{var}_zero_count\"].item()\n",
    "        negative_count = results[f\"{var}_negative_count\"].item()\n",
    "\n",
    "        if nan_count > 0:\n",
    "            flags.append(f\"NaN({nan_count})\")\n",
    "        if inf_count > 0:\n",
    "            flags.append(f\"Inf({inf_count})\")\n",
    "        if zero_count > 0:\n",
    "            flags.append(f\"Zero({zero_count})\")\n",
    "        if negative_count > 0 and config[\"check_negative\"]:\n",
    "            flags.append(f\"Neg({negative_count})\")\n",
    "\n",
    "        return flags\n",
    "\n",
    "    def _create_summary_dataframe(self, all_results: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"Create summary DataFrame from computed results.\"\"\"\n",
    "        summary_data = []\n",
    "\n",
    "        for var, config in self.variable_configs.items():\n",
    "            if f\"{var}_min\" not in all_results:\n",
    "                continue\n",
    "\n",
    "            total_size = all_results[f\"{var}_total_size\"]\n",
    "            nan_count = all_results[f\"{var}_nan_count\"].item()\n",
    "            zero_count = all_results[f\"{var}_zero_count\"].item()\n",
    "            negative_count = all_results[f\"{var}_negative_count\"].item()\n",
    "            inf_count = all_results[f\"{var}_inf_count\"].item()\n",
    "            finite_count = all_results[f\"{var}_finite_count\"].item()\n",
    "\n",
    "            quality_flags = self._create_quality_flags(var, all_results, config)\n",
    "\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"Variable\": var,\n",
    "                    \"Description\": config[\"desc\"],\n",
    "                    \"Min\": all_results[f\"{var}_min\"].item(),\n",
    "                    \"Max\": all_results[f\"{var}_max\"].item(),\n",
    "                    \"Mean\": all_results[f\"{var}_mean\"].item(),\n",
    "                    \"Std\": all_results[f\"{var}_std\"].item(),\n",
    "                    \"Total_Size\": total_size,\n",
    "                    \"Finite_Count\": finite_count,\n",
    "                    \"NaN_Count\": nan_count,\n",
    "                    \"Inf_Count\": inf_count,\n",
    "                    \"Zero_Count\": zero_count,\n",
    "                    \"Negative_Count\": negative_count,\n",
    "                    \"NaN_%\": f\"{nan_count/total_size*100:.3f}%\",\n",
    "                    \"Zero_%\": f\"{zero_count/total_size*100:.3f}%\",\n",
    "                    \"Completeness_%\": f\"{finite_count/total_size*100:.2f}%\",\n",
    "                    \"Data_Quality\": \"; \".join(quality_flags) if quality_flags else \"OK\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "    def _generate_output_filenames(\n",
    "        self, file_url: str, model_date_str: str, model_time_str: str\n",
    "    ) -> Tuple[Path, Path]:\n",
    "        \"\"\"Generate expected output filenames for a given input file.\"\"\"\n",
    "        analysis_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        date_part = (\n",
    "            model_date_str.replace(\"-\", \"\")\n",
    "            if model_date_str != \"Unknown\"\n",
    "            else \"unknown\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            hour_part = pd.to_datetime(f\"{model_date_str} {model_time_str}\").strftime(\n",
    "                \"%H\"\n",
    "            )\n",
    "        except:\n",
    "            hour_part = \"unknown\"\n",
    "\n",
    "        base_filename = f\"weather_stats_{date_part}_{hour_part}_{analysis_timestamp}\"\n",
    "\n",
    "        txt_path = self.output_dir / f\"{base_filename}.txt\"\n",
    "        csv_path = self.output_dir / f\"{base_filename}.csv\"\n",
    "\n",
    "        return txt_path, csv_path\n",
    "\n",
    "    def _check_existing_outputs(\n",
    "        self, file_url: str, overwrite: bool = False\n",
    "    ) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Check if output files already exist for this input file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_url : str\n",
    "            URL of the input file\n",
    "        overwrite : bool\n",
    "            If True, existing files will be overwritten\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[bool, List[str]]\n",
    "            (should_skip, existing_files_list)\n",
    "\n",
    "        \"\"\"\n",
    "        if overwrite:\n",
    "            return False, []\n",
    "\n",
    "        try:\n",
    "            # We need to extract time info to predict the output filename pattern\n",
    "            # This requires opening the file briefly\n",
    "            fs = s3fs.S3FileSystem(\n",
    "                profile=self.s3_profile, config_kwargs={\"max_pool_connections\": 50}\n",
    "            )\n",
    "\n",
    "            with fs.open(file_url, mode=\"rb\") as f:\n",
    "                dataset = xr.open_dataset(f, engine=\"h5netcdf\")\n",
    "                model_date_str, model_time_str = self._extract_time_info(dataset)\n",
    "                dataset.close()\n",
    "\n",
    "            # Generate the expected filename pattern\n",
    "            date_part = (\n",
    "                model_date_str.replace(\"-\", \"\")\n",
    "                if model_date_str != \"Unknown\"\n",
    "                else \"unknown\"\n",
    "            )\n",
    "            try:\n",
    "                hour_part = pd.to_datetime(\n",
    "                    f\"{model_date_str} {model_time_str}\"\n",
    "                ).strftime(\"%H\")\n",
    "            except:\n",
    "                hour_part = \"unknown\"\n",
    "\n",
    "            # Look for existing files with this date_hour pattern\n",
    "            pattern = f\"weather_stats_{date_part}_{hour_part}_*\"\n",
    "            existing_files = list(self.output_dir.glob(pattern + \".txt\"))\n",
    "            existing_csv_files = list(self.output_dir.glob(pattern + \".csv\"))\n",
    "\n",
    "            # If we have both txt and csv files, consider it already processed\n",
    "            if existing_files and existing_csv_files:\n",
    "                return True, [str(f) for f in existing_files + existing_csv_files]\n",
    "\n",
    "            return False, []\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error checking existing outputs for {file_url}: {e}\")\n",
    "            # If we can't check, assume it doesn't exist and process it\n",
    "            return False, []\n",
    "\n",
    "    def _export_results(\n",
    "        self,\n",
    "        summary_df: pd.DataFrame,\n",
    "        file_url: str,\n",
    "        model_date_str: str,\n",
    "        model_time_str: str,\n",
    "        dataset: xr.Dataset,\n",
    "    ) -> Tuple[str, str]:\n",
    "        \"\"\"Export results to text and CSV files.\"\"\"\n",
    "        # Generate output filenames using the new method\n",
    "        txt_path, csv_path = self._generate_output_filenames(\n",
    "            file_url, model_date_str, model_time_str\n",
    "        )\n",
    "\n",
    "        # Export text report\n",
    "        self._export_text_report(\n",
    "            txt_path, summary_df, file_url, model_date_str, model_time_str, dataset\n",
    "        )\n",
    "\n",
    "        # Export CSV\n",
    "        analysis_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self._export_csv_report(\n",
    "            csv_path,\n",
    "            summary_df,\n",
    "            file_url,\n",
    "            model_date_str,\n",
    "            model_time_str,\n",
    "            analysis_timestamp,\n",
    "        )\n",
    "\n",
    "        return str(txt_path), str(csv_path)\n",
    "\n",
    "    def _export_text_report(\n",
    "        self,\n",
    "        file_path: Path,\n",
    "        summary_df: pd.DataFrame,\n",
    "        file_url: str,\n",
    "        model_date_str: str,\n",
    "        model_time_str: str,\n",
    "        dataset: xr.Dataset,\n",
    "    ):\n",
    "        \"\"\"Export detailed text report.\"\"\"\n",
    "        header = f\"\"\"\n",
    "{'='*80}\n",
    "WEATHER DATA STATISTICAL ANALYSIS REPORT\n",
    "{'='*80}\n",
    "Analysis Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Source File: {file_url}\n",
    "Model Date: {model_date_str}\n",
    "Model Time: {model_time_str}\n",
    "\n",
    "Dataset Information:\n",
    "- Dimensions: {dict(dataset.sizes)}\n",
    "- Coordinates: {list(dataset.coords.keys())}\n",
    "- Data Variables: {list(dataset.data_vars.keys())}\n",
    "- Global Attributes: {len(dataset.attrs)} attributes\n",
    "\n",
    "Variable Processing Configuration:\n",
    "{self._format_variable_configs()}\n",
    "{'='*80}\n",
    "\n",
    "STATISTICAL SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(header)\n",
    "\n",
    "            # Write summary table\n",
    "            f.write(\"\\nDETAILED STATISTICS TABLE:\\n\")\n",
    "            f.write(\"=\" * 140 + \"\\n\")\n",
    "\n",
    "            display_df = summary_df.copy()\n",
    "            numerical_cols = [\"Min\", \"Max\", \"Mean\", \"Std\"]\n",
    "            for col in numerical_cols:\n",
    "                display_df[col] = display_df[col].round(6)\n",
    "\n",
    "            f.write(display_df.to_string(index=False, max_colwidth=30))\n",
    "\n",
    "            # Write data quality summary\n",
    "            f.write(f\"\\n\\n{'='*50}\\n\")\n",
    "            f.write(\"DATA QUALITY SUMMARY:\\n\")\n",
    "            f.write(f\"{'='*50}\\n\")\n",
    "\n",
    "            for _, row in summary_df.iterrows():\n",
    "                status = (\n",
    "                    \"✅ No issues detected\"\n",
    "                    if row[\"Data_Quality\"] == \"OK\"\n",
    "                    else row[\"Data_Quality\"]\n",
    "                )\n",
    "                f.write(f\"{row['Variable']}: {status}\\n\")\n",
    "\n",
    "    def _format_variable_configs(self) -> str:\n",
    "        \"\"\"Format variable configurations for display.\"\"\"\n",
    "        config_lines = []\n",
    "        for var, config in self.variable_configs.items():\n",
    "            levels_str = (\n",
    "                \"all levels\"\n",
    "                if config[\"levels\"] == \"all\"\n",
    "                else f\"levels {config['levels']}\"\n",
    "            )\n",
    "            config_lines.append(f\"- {var}: {levels_str}\")\n",
    "        return \"\\n\".join(config_lines)\n",
    "\n",
    "    def _export_csv_report(\n",
    "        self,\n",
    "        file_path: Path,\n",
    "        summary_df: pd.DataFrame,\n",
    "        file_url: str,\n",
    "        model_date_str: str,\n",
    "        model_time_str: str,\n",
    "        analysis_timestamp: str,\n",
    "    ):\n",
    "        \"\"\"Export CSV report with metadata.\"\"\"\n",
    "        csv_df = summary_df.copy()\n",
    "        csv_df.insert(0, \"Source_File\", file_url)\n",
    "        csv_df.insert(1, \"Model_Date\", model_date_str)\n",
    "        csv_df.insert(2, \"Model_Time\", model_time_str)\n",
    "        csv_df.insert(3, \"Analysis_Timestamp\", analysis_timestamp)\n",
    "\n",
    "        csv_df.to_csv(file_path, index=False)\n",
    "\n",
    "    def analyze_file(\n",
    "        self, file_url: str, client: Optional[Client] = None, overwrite: bool = False\n",
    "    ) -> Tuple[pd.DataFrame, str, str]:\n",
    "        \"\"\"Analyze a single weather data file and export statistics.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_url : str\n",
    "            URL or path to the file to analyze\n",
    "        client : Optional[Client]\n",
    "            Existing Dask client to use. If None, creates a new one.\n",
    "        overwrite : bool\n",
    "            If False, skip files that have already been processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[pd.DataFrame, str, str]\n",
    "            Summary DataFrame, text report path, and CSV report path\n",
    "\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Check if file has already been processed\n",
    "        should_skip, existing_files = self._check_existing_outputs(file_url, overwrite)\n",
    "        if should_skip:\n",
    "            self.logger.info(f\"Skipping already processed file: {file_url}\")\n",
    "            self.logger.info(f\"Existing files: {', '.join(existing_files)}\")\n",
    "            # Return the most recent existing files\n",
    "            txt_files = [f for f in existing_files if f.endswith(\".txt\")]\n",
    "            csv_files = [f for f in existing_files if f.endswith(\".csv\")]\n",
    "            if txt_files and csv_files:\n",
    "                # Load summary from existing CSV for consistency\n",
    "                try:\n",
    "                    summary_df = pd.read_csv(csv_files[0])\n",
    "                    # Remove metadata columns to match analyze_file output format\n",
    "                    metadata_cols = [\n",
    "                        \"Source_File\",\n",
    "                        \"Model_Date\",\n",
    "                        \"Model_Time\",\n",
    "                        \"Analysis_Timestamp\",\n",
    "                    ]\n",
    "                    for col in metadata_cols:\n",
    "                        if col in summary_df.columns:\n",
    "                            summary_df = summary_df.drop(columns=[col])\n",
    "                    return summary_df, txt_files[0], csv_files[0]\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Could not load existing summary: {e}\")\n",
    "            # Return empty DataFrame if can't load existing results\n",
    "            return (\n",
    "                pd.DataFrame(),\n",
    "                txt_files[0] if txt_files else \"\",\n",
    "                csv_files[0] if csv_files else \"\",\n",
    "            )\n",
    "\n",
    "        self.logger.info(f\"Processing: {file_url}\")\n",
    "\n",
    "        def _analyze_with_client(dask_client):\n",
    "            \"\"\"Internal function to perform analysis with a given client.\"\"\"\n",
    "            try:\n",
    "                # Initialize S3 filesystem\n",
    "                if self.verbose:\n",
    "                    print(\"  → Connecting to S3...\")\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                fs = s3fs.S3FileSystem(\n",
    "                    profile=self.s3_profile, config_kwargs={\"max_pool_connections\": 50}\n",
    "                )\n",
    "\n",
    "                # Open dataset\n",
    "                if self.verbose:\n",
    "                    print(\"  → Opening dataset...\")\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                with fs.open(file_url, mode=\"rb\") as f:\n",
    "                    dataset = xr.open_dataset(f, engine=\"h5netcdf\", chunks=\"auto\")\n",
    "\n",
    "                # Extract time information\n",
    "                model_date_str, model_time_str = self._extract_time_info(dataset)\n",
    "\n",
    "                # Compute statistics for all variables\n",
    "                all_operations = {}\n",
    "                processed_vars = []\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"  → Preparing computations...\")\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                for var, config in self.variable_configs.items():\n",
    "                    data = self._get_variable_data(dataset, var, config)\n",
    "                    if data is not None:\n",
    "                        stats = self._compute_statistics(data, var, config)\n",
    "                        all_operations.update(stats)\n",
    "                        processed_vars.append(var)\n",
    "\n",
    "                if not all_operations:\n",
    "                    raise ValueError(\"No valid variables found in dataset\")\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"  → Computing statistics...\")\n",
    "                    sys.stdout.flush()\n",
    "                else:\n",
    "                    self.logger.info(\"Computing statistics...\")\n",
    "\n",
    "                results = dask.compute(all_operations)[0]\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"  → Generating reports...\")\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                # Create summary DataFrame\n",
    "                summary_df = self._create_summary_dataframe(results)\n",
    "\n",
    "                # Export results\n",
    "                txt_path, csv_path = self._export_results(\n",
    "                    summary_df, file_url, model_date_str, model_time_str, dataset\n",
    "                )\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"  → Analysis completed successfully\")\n",
    "                    sys.stdout.flush()\n",
    "                else:\n",
    "                    self.logger.info(\"Analysis completed successfully\")\n",
    "\n",
    "                return summary_df, txt_path, csv_path\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"  → Error: {e}\")\n",
    "                    sys.stdout.flush()\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            if client is not None:\n",
    "                # Use provided client\n",
    "                return _analyze_with_client(client)\n",
    "            else:\n",
    "                # Create new client for single file analysis\n",
    "                with Client(\n",
    "                    n_workers=self.n_workers,\n",
    "                    threads_per_worker=2,\n",
    "                    silence_logs=not self.verbose,\n",
    "                ) as new_client:\n",
    "                    return _analyze_with_client(new_client)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during analysis: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 files\n"
     ]
    }
   ],
   "source": [
    "downloader = download.HRESDownloader()\n",
    "hres_dates = downloader.list_matching_keys(\n",
    "            start_date='20240909', end_date='20240911'\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(hres_dates, columns=[\"s3_key\", \"url\"])\n",
    "df[\"dates\"] = df[\"s3_key\"].apply(lambda x: x.split(\"/\")[0])\n",
    "df[\"hour\"] = df[\"s3_key\"].apply(lambda x: x.split(\"/\")[1].split('_')[2][8:10])\n",
    "df[\"filename\"] = df[\"s3_key\"].apply(lambda x: x.split(\"/\")[1])\n",
    "df = df[[\"dates\", \"hour\",\"filename\", \"s3_key\", \"url\"]]\n",
    "\n",
    "print(f\"Found {len(df)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>hour</th>\n",
       "      <th>filename</th>\n",
       "      <th>s3_key</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20240909</td>\n",
       "      <td>00</td>\n",
       "      <td>ECMWF_TROP_202409090000_202409090000_1.nc</td>\n",
       "      <td>20240909/ECMWF_TROP_202409090000_202409090000_...</td>\n",
       "      <td>s3://opera-ecmwf/20240909/ECMWF_TROP_202409090...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20240909</td>\n",
       "      <td>06</td>\n",
       "      <td>ECMWF_TROP_202409090600_202409090600_1.nc</td>\n",
       "      <td>20240909/ECMWF_TROP_202409090600_202409090600_...</td>\n",
       "      <td>s3://opera-ecmwf/20240909/ECMWF_TROP_202409090...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240909</td>\n",
       "      <td>12</td>\n",
       "      <td>ECMWF_TROP_202409091200_202409091200_1.nc</td>\n",
       "      <td>20240909/ECMWF_TROP_202409091200_202409091200_...</td>\n",
       "      <td>s3://opera-ecmwf/20240909/ECMWF_TROP_202409091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20240909</td>\n",
       "      <td>18</td>\n",
       "      <td>ECMWF_TROP_202409091800_202409091800_1.nc</td>\n",
       "      <td>20240909/ECMWF_TROP_202409091800_202409091800_...</td>\n",
       "      <td>s3://opera-ecmwf/20240909/ECMWF_TROP_202409091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20240910</td>\n",
       "      <td>00</td>\n",
       "      <td>ECMWF_TROP_202409100000_202409100000_1.nc</td>\n",
       "      <td>20240910/ECMWF_TROP_202409100000_202409100000_...</td>\n",
       "      <td>s3://opera-ecmwf/20240910/ECMWF_TROP_202409100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20240910</td>\n",
       "      <td>06</td>\n",
       "      <td>ECMWF_TROP_202409100600_202409100600_1.nc</td>\n",
       "      <td>20240910/ECMWF_TROP_202409100600_202409100600_...</td>\n",
       "      <td>s3://opera-ecmwf/20240910/ECMWF_TROP_202409100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20240910</td>\n",
       "      <td>12</td>\n",
       "      <td>ECMWF_TROP_202409101200_202409101200_1.nc</td>\n",
       "      <td>20240910/ECMWF_TROP_202409101200_202409101200_...</td>\n",
       "      <td>s3://opera-ecmwf/20240910/ECMWF_TROP_202409101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20240910</td>\n",
       "      <td>18</td>\n",
       "      <td>ECMWF_TROP_202409101800_202409101800_1.nc</td>\n",
       "      <td>20240910/ECMWF_TROP_202409101800_202409101800_...</td>\n",
       "      <td>s3://opera-ecmwf/20240910/ECMWF_TROP_202409101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20240911</td>\n",
       "      <td>00</td>\n",
       "      <td>ECMWF_TROP_202409110000_202409110000_1.nc</td>\n",
       "      <td>20240911/ECMWF_TROP_202409110000_202409110000_...</td>\n",
       "      <td>s3://opera-ecmwf/20240911/ECMWF_TROP_202409110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20240911</td>\n",
       "      <td>06</td>\n",
       "      <td>ECMWF_TROP_202409110600_202409110600_1.nc</td>\n",
       "      <td>20240911/ECMWF_TROP_202409110600_202409110600_...</td>\n",
       "      <td>s3://opera-ecmwf/20240911/ECMWF_TROP_202409110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20240911</td>\n",
       "      <td>12</td>\n",
       "      <td>ECMWF_TROP_202409111200_202409111200_1.nc</td>\n",
       "      <td>20240911/ECMWF_TROP_202409111200_202409111200_...</td>\n",
       "      <td>s3://opera-ecmwf/20240911/ECMWF_TROP_202409111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20240911</td>\n",
       "      <td>18</td>\n",
       "      <td>ECMWF_TROP_202409111800_202409111800_1.nc</td>\n",
       "      <td>20240911/ECMWF_TROP_202409111800_202409111800_...</td>\n",
       "      <td>s3://opera-ecmwf/20240911/ECMWF_TROP_202409111...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dates hour                                   filename  \\\n",
       "0   20240909   00  ECMWF_TROP_202409090000_202409090000_1.nc   \n",
       "1   20240909   06  ECMWF_TROP_202409090600_202409090600_1.nc   \n",
       "2   20240909   12  ECMWF_TROP_202409091200_202409091200_1.nc   \n",
       "3   20240909   18  ECMWF_TROP_202409091800_202409091800_1.nc   \n",
       "4   20240910   00  ECMWF_TROP_202409100000_202409100000_1.nc   \n",
       "5   20240910   06  ECMWF_TROP_202409100600_202409100600_1.nc   \n",
       "6   20240910   12  ECMWF_TROP_202409101200_202409101200_1.nc   \n",
       "7   20240910   18  ECMWF_TROP_202409101800_202409101800_1.nc   \n",
       "8   20240911   00  ECMWF_TROP_202409110000_202409110000_1.nc   \n",
       "9   20240911   06  ECMWF_TROP_202409110600_202409110600_1.nc   \n",
       "10  20240911   12  ECMWF_TROP_202409111200_202409111200_1.nc   \n",
       "11  20240911   18  ECMWF_TROP_202409111800_202409111800_1.nc   \n",
       "\n",
       "                                               s3_key  \\\n",
       "0   20240909/ECMWF_TROP_202409090000_202409090000_...   \n",
       "1   20240909/ECMWF_TROP_202409090600_202409090600_...   \n",
       "2   20240909/ECMWF_TROP_202409091200_202409091200_...   \n",
       "3   20240909/ECMWF_TROP_202409091800_202409091800_...   \n",
       "4   20240910/ECMWF_TROP_202409100000_202409100000_...   \n",
       "5   20240910/ECMWF_TROP_202409100600_202409100600_...   \n",
       "6   20240910/ECMWF_TROP_202409101200_202409101200_...   \n",
       "7   20240910/ECMWF_TROP_202409101800_202409101800_...   \n",
       "8   20240911/ECMWF_TROP_202409110000_202409110000_...   \n",
       "9   20240911/ECMWF_TROP_202409110600_202409110600_...   \n",
       "10  20240911/ECMWF_TROP_202409111200_202409111200_...   \n",
       "11  20240911/ECMWF_TROP_202409111800_202409111800_...   \n",
       "\n",
       "                                                  url  \n",
       "0   s3://opera-ecmwf/20240909/ECMWF_TROP_202409090...  \n",
       "1   s3://opera-ecmwf/20240909/ECMWF_TROP_202409090...  \n",
       "2   s3://opera-ecmwf/20240909/ECMWF_TROP_202409091...  \n",
       "3   s3://opera-ecmwf/20240909/ECMWF_TROP_202409091...  \n",
       "4   s3://opera-ecmwf/20240910/ECMWF_TROP_202409100...  \n",
       "5   s3://opera-ecmwf/20240910/ECMWF_TROP_202409100...  \n",
       "6   s3://opera-ecmwf/20240910/ECMWF_TROP_202409101...  \n",
       "7   s3://opera-ecmwf/20240910/ECMWF_TROP_202409101...  \n",
       "8   s3://opera-ecmwf/20240911/ECMWF_TROP_202409110...  \n",
       "9   s3://opera-ecmwf/20240911/ECMWF_TROP_202409110...  \n",
       "10  s3://opera-ecmwf/20240911/ECMWF_TROP_202409111...  \n",
       "11  s3://opera-ecmwf/20240911/ECMWF_TROP_202409111...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = df[(df.dates == '20240910') & (df.hour == '06')].url.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/aurora-r0/govorcin/miniconda/miniforge/envs/opera_tropo/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43869 instead\n",
      "  warnings.warn(\n",
      "2025-06-26 16:24:53,290 - distributed.scheduler - INFO - State start\n",
      "2025-06-26 16:24:53,305 - distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,306 - distributed.scheduler - INFO -   dashboard at:  http://127.0.0.1:43869/status\n",
      "2025-06-26 16:24:53,306 - distributed.scheduler - INFO - Registering Worker plugin shuffle\n",
      "2025-06-26 16:24:53,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42461'\n",
      "2025-06-26 16:24:53,342 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33160'\n",
      "2025-06-26 16:24:53,345 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44157'\n",
      "2025-06-26 16:24:53,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46055'\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38845\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38845\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -           Worker name:                          0\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33196\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -               Threads:                          2\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -                Memory:                   1.86 GiB\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-75956/worker-qymoau2r\n",
      "2025-06-26 16:24:53,763 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35760\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35760\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -           Worker name:                          2\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33338\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -               Threads:                          2\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -                Memory:                   1.86 GiB\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-75956/worker-6wlc83bi\n",
      "2025-06-26 16:24:53,773 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,776 - distributed.scheduler - INFO - Register worker addr: tcp://127.0.0.1:38845 name: 0\n",
      "2025-06-26 16:24:53,777 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38845\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35977\n",
      "2025-06-26 16:24:53,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44162\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35977\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -           Worker name:                          3\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34349\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -               Threads:                          2\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -                Memory:                   1.86 GiB\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-75956/worker-uvsxzhwo\n",
      "2025-06-26 16:24:53,777 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,778 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-06-26 16:24:53,778 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,778 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,781 - distributed.scheduler - INFO - Register worker addr: tcp://127.0.0.1:35760 name: 2\n",
      "2025-06-26 16:24:53,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35760\n",
      "2025-06-26 16:24:53,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44172\n",
      "2025-06-26 16:24:53,782 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-06-26 16:24:53,782 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,782 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,786 - distributed.scheduler - INFO - Register worker addr: tcp://127.0.0.1:35977 name: 3\n",
      "2025-06-26 16:24:53,787 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35977\n",
      "2025-06-26 16:24:53,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44176\n",
      "2025-06-26 16:24:53,787 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-06-26 16:24:53,788 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,788 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42900\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42900\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -           Worker name:                          1\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36432\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -               Threads:                          2\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -                Memory:                   1.86 GiB\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-75956/worker-ycuarlgn\n",
      "2025-06-26 16:24:53,827 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,835 - distributed.scheduler - INFO - Register worker addr: tcp://127.0.0.1:42900 name: 1\n",
      "2025-06-26 16:24:53,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42900\n",
      "2025-06-26 16:24:53,837 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-06-26 16:24:53,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44222\n",
      "2025-06-26 16:24:53,837 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,837 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-06-26 16:24:53,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32905\n",
      "2025-06-26 16:24:53,870 - distributed.scheduler - INFO - Receive client connection: Client-c3604ef5-52e4-11f0-99b1-e43d1adad770\n",
      "2025-06-26 16:24:53,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44258\n",
      "2025-06-26 16:26:05,490 - INFO - Processing: s3://opera-ecmwf/20240910/ECMWF_TROP_202409100600_202409100600_1.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Connecting to S3...\n",
      "  → Opening dataset...\n",
      "  → Preparing computations...\n",
      "  → Computing statistics...\n",
      "  → Generating reports...\n",
      "  → Analysis completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 16:27:56,502 - distributed.scheduler - INFO - Remove client Client-c3604ef5-52e4-11f0-99b1-e43d1adad770\n",
      "2025-06-26 16:27:56,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44258; closing.\n",
      "2025-06-26 16:27:56,504 - distributed.scheduler - INFO - Remove client Client-c3604ef5-52e4-11f0-99b1-e43d1adad770\n",
      "2025-06-26 16:27:56,505 - distributed.scheduler - INFO - Close client connection: Client-c3604ef5-52e4-11f0-99b1-e43d1adad770\n",
      "2025-06-26 16:27:56,506 - distributed.scheduler - INFO - Retire worker addresses (stimulus_id='retire-workers-1750980476.5067892') (0, 1, 2, 3)\n",
      "2025-06-26 16:27:56,507 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42461'. Reason: nanny-close\n",
      "2025-06-26 16:27:56,508 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2025-06-26 16:27:56,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33160'. Reason: nanny-close\n",
      "2025-06-26 16:27:56,510 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2025-06-26 16:27:56,510 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44157'. Reason: nanny-close\n",
      "2025-06-26 16:27:56,510 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38845. Reason: nanny-close\n",
      "2025-06-26 16:27:56,511 - distributed.worker - INFO - Removing Worker plugin shuffle\n",
      "2025-06-26 16:27:56,511 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2025-06-26 16:27:56,511 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42900. Reason: nanny-close\n",
      "2025-06-26 16:27:56,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46055'. Reason: nanny-close\n",
      "2025-06-26 16:27:56,512 - distributed.worker - INFO - Removing Worker plugin shuffle\n",
      "2025-06-26 16:27:56,513 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2025-06-26 16:27:56,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44222; closing.\n",
      "2025-06-26 16:27:56,514 - distributed.core - INFO - Connection to tcp://127.0.0.1:32905 has been closed.\n",
      "2025-06-26 16:27:56,516 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35760. Reason: nanny-close\n",
      "2025-06-26 16:27:56,516 - distributed.scheduler - INFO - Remove worker addr: tcp://127.0.0.1:42900 name: 1 (stimulus_id='handle-worker-cleanup-1750980476.5168407')\n",
      "2025-06-26 16:27:56,516 - distributed.worker - INFO - Removing Worker plugin shuffle\n",
      "2025-06-26 16:27:56,516 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35977. Reason: nanny-close\n",
      "2025-06-26 16:27:56,517 - distributed.worker - INFO - Removing Worker plugin shuffle\n",
      "2025-06-26 16:27:56,516 - distributed.core - INFO - Connection to tcp://127.0.0.1:32905 has been closed.\n",
      "2025-06-26 16:27:56,517 - distributed.core - INFO - Connection to tcp://127.0.0.1:32905 has been closed.\n",
      "2025-06-26 16:27:56,518 - distributed.nanny - INFO - Worker closed\n",
      "2025-06-26 16:27:56,518 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44162; closing.\n",
      "2025-06-26 16:27:56,520 - distributed.scheduler - INFO - Remove worker addr: tcp://127.0.0.1:38845 name: 0 (stimulus_id='handle-worker-cleanup-1750980476.5201666')\n",
      "2025-06-26 16:27:56,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44172; closing.\n",
      "2025-06-26 16:27:56,520 - distributed.core - INFO - Connection to tcp://127.0.0.1:32905 has been closed.\n",
      "2025-06-26 16:27:56,521 - distributed.nanny - INFO - Worker closed\n",
      "2025-06-26 16:27:56,521 - distributed.nanny - INFO - Worker closed\n",
      "2025-06-26 16:27:56,522 - distributed.scheduler - INFO - Remove worker addr: tcp://127.0.0.1:35760 name: 2 (stimulus_id='handle-worker-cleanup-1750980476.5219786')\n",
      "2025-06-26 16:27:56,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44176; closing.\n",
      "2025-06-26 16:27:56,522 - distributed.nanny - INFO - Worker closed\n",
      "2025-06-26 16:27:56,523 - distributed.scheduler - INFO - Remove worker addr: tcp://127.0.0.1:35977 name: 3 (stimulus_id='handle-worker-cleanup-1750980476.523689')\n",
      "2025-06-26 16:27:56,524 - distributed.scheduler - INFO - Lost all workers\n",
      "2025-06-26 16:27:56,772 - distributed.nanny - INFO - Nanny at 'tcp://127.0.0.1:33160' closed.\n",
      "2025-06-26 16:27:56,950 - distributed.nanny - INFO - Nanny at 'tcp://127.0.0.1:42461' closed.\n",
      "2025-06-26 16:27:56,975 - distributed.nanny - INFO - Nanny at 'tcp://127.0.0.1:44157' closed.\n",
      "2025-06-26 16:27:56,978 - distributed.nanny - INFO - Nanny at 'tcp://127.0.0.1:46055' closed.\n",
      "2025-06-26 16:27:56,979 - distributed.scheduler - INFO - Closing scheduler. Reason: unknown\n",
      "2025-06-26 16:27:56,980 - distributed.scheduler - INFO - Scheduler closing all comms\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "analyzer = WeatherDataAnalyzer(\n",
    "    output_dir=Path('download_test'),\n",
    "    n_workers=3,\n",
    "    s3_profile=\"saml-pub\",\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "\n",
    "with Client(\n",
    "            n_workers=4,\n",
    "            threads_per_worker=2,\n",
    "            memory_limit=f\"{2}GB\",\n",
    "            silence_logs=not verbose,\n",
    "        ) as client:\n",
    "    \n",
    "    should_skip, existing_files = (\n",
    "                            analyzer._check_existing_outputs(file_url, overwrite=False)\n",
    "                            )\n",
    "    summary_df, txt_file, csv_file = analyzer.analyze_file(\n",
    "                            file_url, client=client, overwrite=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Description</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Total_Size</th>\n",
       "      <th>Finite_Count</th>\n",
       "      <th>NaN_Count</th>\n",
       "      <th>Inf_Count</th>\n",
       "      <th>Zero_Count</th>\n",
       "      <th>Negative_Count</th>\n",
       "      <th>NaN_%</th>\n",
       "      <th>Zero_%</th>\n",
       "      <th>Completeness_%</th>\n",
       "      <th>Data_Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z</td>\n",
       "      <td>Geopotential (surface)</td>\n",
       "      <td>-4.563282e+03</td>\n",
       "      <td>62700.718750</td>\n",
       "      <td>3712.363525</td>\n",
       "      <td>8367.480469</td>\n",
       "      <td>13107200</td>\n",
       "      <td>13107200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t</td>\n",
       "      <td>Temperature (all levels)</td>\n",
       "      <td>1.821989e+02</td>\n",
       "      <td>317.135040</td>\n",
       "      <td>240.076660</td>\n",
       "      <td>29.328047</td>\n",
       "      <td>1795686400</td>\n",
       "      <td>1795686400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q</td>\n",
       "      <td>Specific humidity (all levels)</td>\n",
       "      <td>2.282539e-08</td>\n",
       "      <td>0.028652</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>1795686400</td>\n",
       "      <td>1795686400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lnsp</td>\n",
       "      <td>Log surface pressure</td>\n",
       "      <td>1.074795e+01</td>\n",
       "      <td>11.574851</td>\n",
       "      <td>11.472398</td>\n",
       "      <td>0.117171</td>\n",
       "      <td>13107200</td>\n",
       "      <td>13107200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>0.000%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable                     Description           Min           Max  \\\n",
       "0        z          Geopotential (surface) -4.563282e+03  62700.718750   \n",
       "1        t        Temperature (all levels)  1.821989e+02    317.135040   \n",
       "2        q  Specific humidity (all levels)  2.282539e-08      0.028652   \n",
       "3     lnsp            Log surface pressure  1.074795e+01     11.574851   \n",
       "\n",
       "          Mean          Std  Total_Size  Finite_Count  NaN_Count  Inf_Count  \\\n",
       "0  3712.363525  8367.480469    13107200      13107200          0          0   \n",
       "1   240.076660    29.328047  1795686400    1795686400          0          0   \n",
       "2     0.001634     0.003674  1795686400    1795686400          0          0   \n",
       "3    11.472398     0.117171    13107200      13107200          0          0   \n",
       "\n",
       "   Zero_Count  Negative_Count   NaN_%  Zero_% Completeness_% Data_Quality  \n",
       "0           0               0  0.000%  0.000%        100.00%           OK  \n",
       "1           0               0  0.000%  0.000%        100.00%           OK  \n",
       "2           0               0  0.000%  0.000%        100.00%           OK  \n",
       "3           0               0  0.000%  0.000%        100.00%           OK  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opera_tropo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
